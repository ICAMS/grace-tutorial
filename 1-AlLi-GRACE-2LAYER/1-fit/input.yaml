seed: 1
cutoff: 6.0

data:
  filename: ../0-data/collected.pkl.gz
  test_size: 0.05
  reference_energy: 0
  # reference_energy: {Al: -1.23, Li: -3.56}
  save_dataset: False
  # stress_units: eV/A3 # eV/A3 (default) or GPa or kbar or -kbar


potential:
  # elements: [Cu, Al] # If elements not provided - determined automatically from data
  preset: GRACE_2LAYER_latest

  ## For custom model from model.py::custom_model
  #  custom: model.custom_model

  # keyword-arguments passed to preset or custom function
  kwargs: {lmax: [4, 3], max_order: 4, indicator_lmax: 1, n_rad_max: [32, 32], prod_func_n_max: [32, 32], n_mlp_dens: 12, n_rad_base: 8}

  #shift: False # True/False
  scale: True  # False/True or float
fit:
  loss: {
    energy: { weight: 16, type: huber , delta: 0.01 },
    forces: { weight: 32, type: huber , delta: 0.01 },
    stress: { weight: 128.0, type: huber, delta: 0.01 },
    switch: { after_iter: 0.75, learning_rate_reduction_factor: 0.1, energy: { weight: 128 }, forces: { weight: 32 }, stress: { weight: 128.0 }, }
  }

  

  maxiter: auto # Number of epochs / iterations. auto: ~50k updates (scratch) or ~10k (finetune)
  target_total_updates: 5000 # alternative: specify total gradient updates instead of epochs
  # Switch E/F loss weights mid-training: after_iter can be fraction (0.75 = 75% of maxiter) or int
  # learning_rate_reduction_factor: 0.1 → new LR = current_LR × factor  (preferred over absolute learning_rate)

  optimizer: Adam
  opt_params: {
            learning_rate: 0.008,
            use_ema: True,
            ema_momentum: 0.99,
            weight_decay: null,
            clipnorm: 1.0,
        }

  scheduler: reduce_on_plateau # scheduler for learning-rate reduction during training
  scheduler_params: { "patience": 50, "reduction_factor": 0.8, "minimum_learning_rate": 0.0006666666666666666, "stop_at_min": True }
  #  optimizer: L-BFGS-B
  #  opt_params: { "maxcor": 100, "maxls": 20 }

  ## needed for low-energy tier metrics and for "convex_hull"-based distance of energy-based weighting scheme
  compute_convex_hull: False
  batch_size: 8 # Important hyperparameter for Adam and irrelevant (but must be) for L-BFGS-B
  test_batch_size: 32 # test batch size (optional)

  jit_compile: True
  eval_init_stats: False # to evaluate initial metrics

  train_max_n_buckets: auto # max number of buckets (group of batches of same shape) in train set
  test_max_n_buckets: auto # same for test
  auto_bucket_max_padding: 0.3 # max allowed padding overhead for neighbours when using 'auto' buckets

  checkpoint_freq: 10 # frequency for **REGULAR** checkpoints.
  # save_all_regular_checkpoints: True # to store ALL regular checkpoints
  progressbar: False # show batch-evaluation progress bar
  train_shuffle: True # shuffle train batches on every epoch

